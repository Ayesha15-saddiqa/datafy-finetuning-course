{"cells":[{"cell_type":"code","source":["# Install python packages\n","! pip install datasets\n","! pip install transformers"],"metadata":{"id":"kOA8S4R_Xl22"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Assuming, you have cloned the Repo in first step"],"metadata":{"id":"OP3M5QaoXmzf"}},{"cell_type":"code","source":["# Mount the Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# clone if not done before\n","# !git clone https://github.com/amjadraza/datafy-finetuning-course.git /content/drive/MyDrive/datafy-finetuning-course\n","\n","%cd drive/MyDrive/datafy-finetuning-course"],"metadata":{"id":"92I7z-bQXnS5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L-NxmfuOT5yB"},"source":["# Data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSEqgeA468du"},"outputs":[],"source":["import pandas as pd\n","import datasets\n","\n","from pprint import pprint\n","from transformers import AutoTokenizer"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"cE1fb9c9XbaQ"},"source":["### Tokenizing text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LYavrokXbaR"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmsCkkbhXbaR"},"outputs":[],"source":["text = \"Hi, how are you?\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9K5w-CtTXbaR"},"outputs":[],"source":["encoded_text = tokenizer(text)[\"input_ids\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XG7cHLf3XbaS"},"outputs":[],"source":["encoded_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"705pK629XbaS"},"outputs":[],"source":["decoded_text = tokenizer.decode(encoded_text)\n","print(\"Decoded tokens back into text: \", decoded_text)"]},{"cell_type":"markdown","metadata":{"id":"BfRc_aCaXbaS"},"source":["### Tokenize multiple texts at once"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgJnTh8RXbaS"},"outputs":[],"source":["list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n","encoded_texts = tokenizer(list_texts)\n","print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"]},{"cell_type":"markdown","metadata":{"id":"R0vrMwdtXbaT"},"source":["### Padding and truncation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAI6kTM0XbaT"},"outputs":[],"source":["tokenizer.pad_token = tokenizer.eos_token\n","encoded_texts_longest = tokenizer(list_texts, padding=True)\n","print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kclgZ6b0XbaT"},"outputs":[],"source":["encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n","print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H8mTsM7kXbaU"},"outputs":[],"source":["tokenizer.truncation_side = \"left\"\n","encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n","print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-cA4GWwpXbaU"},"outputs":[],"source":["encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n","print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"]},{"cell_type":"markdown","metadata":{"id":"lZMXSCrrXbaU"},"source":["### Prepare instruction dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"ZWzlKqI-XbaU"},"outputs":[],"source":["import pandas as pd\n","\n","filename = \"notebooks/lamini_docs.jsonl\"\n","instruction_dataset_df = pd.read_json(filename, lines=True)\n","examples = instruction_dataset_df.to_dict()\n","\n","if \"question\" in examples and \"answer\" in examples:\n","  text = examples[\"question\"][0] + examples[\"answer\"][0]\n","elif \"instruction\" in examples and \"response\" in examples:\n","  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n","elif \"input\" in examples and \"output\" in examples:\n","  text = examples[\"input\"][0] + examples[\"output\"][0]\n","else:\n","  text = examples[\"text\"][0]\n","\n","prompt_template = \"\"\"### Question:\n","{question}\n","\n","### Answer:\"\"\"\n","\n","num_examples = len(examples[\"question\"])\n","finetuning_dataset = []\n","for i in range(num_examples):\n","  question = examples[\"question\"][i]\n","  answer = examples[\"answer\"][i]\n","  text_with_prompt_template = prompt_template.format(question=question)\n","  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n","\n","from pprint import pprint\n","print(\"One datapoint in the finetuning dataset:\")\n","pprint(finetuning_dataset[0])"]},{"cell_type":"markdown","metadata":{"id":"F6P7WVKkXbaV"},"source":["### Tokenize a single example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sr3OPdpXbaV"},"outputs":[],"source":["text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n","tokenized_inputs = tokenizer(\n","    text,\n","    return_tensors=\"np\",\n","    padding=True\n",")\n","print(tokenized_inputs[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0NTDxVtXbaV"},"outputs":[],"source":["max_length = 2048\n","max_length = min(\n","    tokenized_inputs[\"input_ids\"].shape[1],\n","    max_length,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsvhbM-oXbaV"},"outputs":[],"source":["tokenized_inputs = tokenizer(\n","    text,\n","    return_tensors=\"np\",\n","    truncation=True,\n","    max_length=max_length\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YnnPViedXbaV"},"outputs":[],"source":["tokenized_inputs[\"input_ids\"]"]},{"cell_type":"markdown","metadata":{"id":"Y2bxOvszXbaW"},"source":["### Tokenize the instruction dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y6mUD8RzXbaW"},"outputs":[],"source":["def tokenize_function(examples):\n","    if \"question\" in examples and \"answer\" in examples:\n","      text = examples[\"question\"][0] + examples[\"answer\"][0]\n","    elif \"input\" in examples and \"output\" in examples:\n","      text = examples[\"input\"][0] + examples[\"output\"][0]\n","    else:\n","      text = examples[\"text\"][0]\n","\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenized_inputs = tokenizer(\n","        text,\n","        return_tensors=\"np\",\n","        padding=True,\n","    )\n","\n","    max_length = min(\n","        tokenized_inputs[\"input_ids\"].shape[1],\n","        2048\n","    )\n","    tokenizer.truncation_side = \"left\"\n","    tokenized_inputs = tokenizer(\n","        text,\n","        return_tensors=\"np\",\n","        truncation=True,\n","        max_length=max_length\n","    )\n","\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_jNxeBKXbaW"},"outputs":[],"source":["finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n","\n","tokenized_dataset = finetuning_dataset_loaded.map(\n","    tokenize_function,\n","    batched=True,\n","    batch_size=1,\n","    drop_last_batch=True\n",")\n","\n","print(tokenized_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kkfyuCtgXbaW"},"outputs":[],"source":["tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"]},{"cell_type":"markdown","metadata":{"id":"8MV-jZRcXbaW"},"source":["### Prepare test/train splits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qes27LuJXbaW"},"outputs":[],"source":["split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n","print(split_dataset)"]},{"cell_type":"markdown","metadata":{"id":"_O2zL1S4XbaW"},"source":["### Some datasets for you to try"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5yFuRGS4XbaW"},"outputs":[],"source":["finetuning_dataset_path = \"lamini/lamini_docs\"\n","finetuning_dataset = datasets.load_dataset(finetuning_dataset_path)\n","print(finetuning_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sokaGcSJXbaX"},"outputs":[],"source":["taylor_swift_dataset = \"lamini/taylor_swift\"\n","bts_dataset = \"lamini/bts\"\n","open_llms = \"lamini/open_llms\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yf-42pB1XbaX"},"outputs":[],"source":["dataset_swiftie = datasets.load_dataset(taylor_swift_dataset)\n","print(dataset_swiftie[\"train\"][1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5jQayVfSOP0"},"outputs":[],"source":["# This is how to push your own dataset to your Huggingface hub\n","# !pip install huggingface_hub\n","# !huggingface-cli login\n","# split_dataset.push_to_hub(dataset_path_hf)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}